# Enhancing Sentiment Classification Using Engineered Text Features
*A Case Study on Amazon Food Reviews*

## ğŸ§  Project Overview

This project explores how engineered sentiment and linguistic features can improve sentiment classification performance in food product reviews. The analysis uses the **Amazon Fine Food Reviews** dataset and compares multiple classification models including **Logistic Regression**, **Random Forest**, and **Naive Bayes**.

Key contributions:
- Advanced feature engineering (polarity, subjectivity, structural cues)
- Rigorous model tuning and cross-validation
- Reproducibility and interpretability

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data/               # Input data files (not uploaded to GitHub)
â”œâ”€â”€ notebooks/          # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ Feature Engineering and EDA.ipynb
â”‚   â”œâ”€â”€ Comparison with Aravindan.ipynb
â”‚   â””â”€â”€ Comparison with Landge.ipynb
â”œâ”€â”€ src/                # Python scripts for preprocessing, modeling (to be added)
â”œâ”€â”€ models/             # Saved trained models (optional)
â”œâ”€â”€ results/            # Evaluation outputs, metrics
â”œâ”€â”€ figures/            # Visualizations used in the thesis
â”œâ”€â”€ reports/            # Project summary or thesis PDF link
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ README.md           # This file
â””â”€â”€ .gitignore
```

## ğŸ“Š Dataset

- **Name**: Amazon Fine Food Reviews  
- **Source**: [Kaggle](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews)  
- **Size**: 568,454 reviews from 256,059 users on 74,258 products  
- **Note**: Neutral reviews (score = 3) were excluded for binary classification

## âš™ï¸ How to Run

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/sentiment-thesis.git
cd sentiment-thesis
```

### 2. Set up environment
Using pip:
```bash
pip install -r requirements.txt
```

### 3. Run notebooks
Open the notebooks in the `notebooks/` folder using Jupyter Lab or Jupyter Notebook to explore feature engineering and model comparison.

## ğŸ—ï¸ Models Used

- Logistic Regression
- Random Forest
- Gaussian Naive Bayes

Hyperparameter tuning was done using `GridSearchCV` with 5-fold cross-validation.

## ğŸ“ˆ Results

| Model            | Accuracy | Precision | Recall | F1-score |
|------------------|----------|-----------|--------|----------|
| Random Forest    | 0.88     | 0.90      | 0.97   | **0.93** |
| Logistic Regr.   | 0.87     | 0.87      | 0.98   | 0.92     |
| Naive Bayes      | 0.84     | 0.87      | 0.96   | 0.91     |

Top-N evaluation (N=10) also showed **Random Forest** achieving highest precision and recall.

## ğŸ§ª Feature Engineering

Engineered features include:
- `SentimentText_Polarity`
- `SentimentText_Subjectivity`
- `TextLengthWords`
- `NumUppercaseWords`
- `AvgWordLength`
- `SentimentDelta` (difference between text and summary polarity)

These are implemented in the `Feature Engineering and EDA.ipynb` notebook.

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.

## ğŸ¤ Acknowledgements

This project was conducted as part of the **MSc Data Science & Society** program at **Tilburg University**.  
Supervisor: Prof. Dr. Eric Postma  
Dataset: McAuley (2012) â€“ Amazon Fine Food Reviews  
Sentiment analysis: TextBlob (Loria, 2018)

## ğŸ“¬ Contact

For questions, reach out via GitHub Issues or contact me at `rickvdb@example.com`.
