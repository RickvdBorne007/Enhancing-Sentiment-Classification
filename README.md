# Enhancing Sentiment Classification Using Engineered Text Features
*A Case Study on Amazon Food Reviews*

## ğŸ§  Project Overview

This project explores how engineered sentiment and linguistic features can improve sentiment classification performance in food product reviews. The analysis uses the **Amazon Fine Food Reviews** dataset and compares multiple classification models including **Logistic Regression**, **Random Forest**, and **Naive Bayes**.

Key contributions:
- Advanced feature engineering (polarity, subjectivity, structural cues)
- Rigorous model tuning and cross-validation
- Reproducibility and interpretability

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data/               # Input data files (not uploaded to GitHub)
â”œâ”€â”€ notebooks/          # Jupyter notebooks (EDA, training, evaluation)
â”œâ”€â”€ src/                # Python scripts for preprocessing, modeling
â”œâ”€â”€ models/             # Saved trained models
â”œâ”€â”€ results/            # Evaluation outputs, metrics
â”œâ”€â”€ figures/            # Visualizations used in the thesis
â”œâ”€â”€ reports/            # Project summary or thesis PDF link
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ environment.yml     # (optional) Conda environment
â”œâ”€â”€ README.md           # This file
â””â”€â”€ .gitignore
```

## ğŸ“Š Dataset

- **Name**: Amazon Fine Food Reviews  
- **Source**: [Kaggle](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews)  
- **Size**: 568,454 reviews from 256,059 users on 74,258 products  

## âš™ï¸ How to Run

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/sentiment-thesis.git
cd sentiment-thesis
```

### 2. Set up environment
Using pip:
```bash
pip install -r requirements.txt
```

Or with Conda:
```bash
conda env create -f environment.yml
conda activate sentiment-thesis
```

### 3. Run the pipeline
Scripts can be run via CLI (see `src/train_model.py`), or explore the process via the Jupyter notebooks in the `notebooks/` folder.

## ğŸ—ï¸ Models Used

- Logistic Regression
- Random Forest
- Gaussian Naive Bayes

Hyperparameter tuning was done using `GridSearchCV` with 5-fold cross-validation.

## ğŸ“ˆ Results

| Model            | Accuracy | Precision | Recall | F1-score |
|------------------|----------|-----------|--------|----------|
| Random Forest    | 0.88     | 0.90      | 0.97   | **0.93** |
| Logistic Regr.   | 0.87     | 0.87      | 0.98   | 0.92     |
| Naive Bayes      | 0.84     | 0.87      | 0.96   | 0.91     |

Top-N evaluation (N=10) also showed **Random Forest** achieving highest precision and recall.

## ğŸ§ª Feature Engineering

Engineered features include:
- `SentimentText_Polarity`
- `SentimentText_Subjectivity`
- `TextLengthWords`
- `NumUppercaseWords`
- `AvgWordLength`

See `src/features.py` for full logic.

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.

## ğŸ¤ Acknowledgements

This project was conducted as part of the **MSc Data Science & Society** program at **Tilburg University**.  
Supervisor: Prof. Dr. Eric Postma  
Dataset: McAuley (2012) â€“ Amazon Fine Food Reviews  
Sentiment analysis: TextBlob (Loria, 2018)

## ğŸ“¬ Contact

For questions, reach out via GitHub Issues or contact me at `r.h.m.vdnborne@tilburguniversity.edu`.

